{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet sentiment prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gensim\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import os\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction from each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets: 15513\n"
     ]
    }
   ],
   "source": [
    "# Reading in all files\n",
    "lines = []\n",
    "file_path = 'tweets/'\n",
    "files = [os.path.join(file_path, x) for x in os.listdir(file_path)]\n",
    "for fle in files:\n",
    "    tweets_file = open(fle, 'r')\n",
    "    _temp = tweets_file.readlines()\n",
    "    lines += _temp\n",
    "    tweets_file.close()\n",
    "print (\"Number of tweets: %d\" % len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'contributors': None,\n",
      " 'coordinates': None,\n",
      " 'created_at': 'Tue Aug 15 17:58:26 +0000 2017',\n",
      " 'entities': {'hashtags': [{'indices': [45, 52], 'text': 'patent'},\n",
      "                           {'indices': [65, 68], 'text': 'IP'}],\n",
      "              'symbols': [{'indices': [103, 108], 'text': 'GOOG'},\n",
      "                          {'indices': [109, 112], 'text': 'FB'}],\n",
      "              'urls': [{'display_url': 'iam-media.com/blog/Detail.as…',\n",
      "                        'expanded_url': 'http://www.iam-media.com/blog/Detail.aspx?g=afc6cc58-706a-475d-906a-fd85bd1e49f1',\n",
      "                        'indices': [113, 136],\n",
      "                        'url': 'https://t.co/FiHWRiETq3'}],\n",
      "              'user_mentions': [{'id': 108564136,\n",
      "                                 'id_str': '108564136',\n",
      "                                 'indices': [3, 16],\n",
      "                                 'name': 'IAM',\n",
      "                                 'screen_name': 'IAM_magazine'}]},\n",
      " 'favorite_count': 0,\n",
      " 'favorited': False,\n",
      " 'geo': None,\n",
      " 'id': 897517856702812160,\n",
      " 'id_str': '897517856702812160',\n",
      " 'in_reply_to_screen_name': None,\n",
      " 'in_reply_to_status_id': None,\n",
      " 'in_reply_to_status_id_str': None,\n",
      " 'in_reply_to_user_id': None,\n",
      " 'in_reply_to_user_id_str': None,\n",
      " 'is_quote_status': False,\n",
      " 'lang': 'en',\n",
      " 'metadata': {'iso_language_code': 'en', 'result_type': 'recent'},\n",
      " 'place': None,\n",
      " 'possibly_sensitive': False,\n",
      " 'retweet_count': 9,\n",
      " 'retweeted': False,\n",
      " 'retweeted_status': {'contributors': None,\n",
      "                      'coordinates': None,\n",
      "                      'created_at': 'Mon Aug 14 18:37:19 +0000 2017',\n",
      "                      'entities': {'hashtags': [{'indices': [27, 34],\n",
      "                                                 'text': 'patent'},\n",
      "                                                {'indices': [47, 50],\n",
      "                                                 'text': 'IP'}],\n",
      "                                   'symbols': [{'indices': [85, 90],\n",
      "                                                'text': 'GOOG'},\n",
      "                                               {'indices': [91, 94],\n",
      "                                                'text': 'FB'}],\n",
      "                                   'urls': [{'display_url': 'iam-media.com/blog/Detail.as…',\n",
      "                                             'expanded_url': 'http://www.iam-media.com/blog/Detail.aspx?g=afc6cc58-706a-475d-906a-fd85bd1e49f1',\n",
      "                                             'indices': [95, 118],\n",
      "                                             'url': 'https://t.co/FiHWRiETq3'}],\n",
      "                                   'user_mentions': []},\n",
      "                      'favorite_count': 4,\n",
      "                      'favorited': False,\n",
      "                      'geo': None,\n",
      "                      'id': 897165251254382593,\n",
      "                      'id_str': '897165251254382593',\n",
      "                      'in_reply_to_screen_name': None,\n",
      "                      'in_reply_to_status_id': None,\n",
      "                      'in_reply_to_status_id_str': None,\n",
      "                      'in_reply_to_user_id': None,\n",
      "                      'in_reply_to_user_id_str': None,\n",
      "                      'is_quote_status': False,\n",
      "                      'lang': 'en',\n",
      "                      'metadata': {'iso_language_code': 'en',\n",
      "                                   'result_type': 'recent'},\n",
      "                      'place': None,\n",
      "                      'possibly_sensitive': False,\n",
      "                      'retweet_count': 9,\n",
      "                      'retweeted': False,\n",
      "                      'source': '<a href=\"http://twitter.com\" '\n",
      "                                'rel=\"nofollow\">Twitter Web Client</a>',\n",
      "                      'text': 'Exclusive: In major Valley #patent move Google '\n",
      "                              '#IP head Allen Lo is joining Facebook $GOOG $FB '\n",
      "                              'https://t.co/FiHWRiETq3',\n",
      "                      'truncated': False,\n",
      "                      'user': {'contributors_enabled': False,\n",
      "                               'created_at': 'Tue Jan 26 09:55:36 +0000 2010',\n",
      "                               'default_profile': False,\n",
      "                               'default_profile_image': False,\n",
      "                               'description': 'Please note: Retweets should '\n",
      "                                              'not be regarded as '\n",
      "                                              'endorsements.',\n",
      "                               'entities': {'description': {'urls': []},\n",
      "                                            'url': {'urls': [{'display_url': 'iam-media.com',\n",
      "                                                              'expanded_url': 'http://www.iam-media.com',\n",
      "                                                              'indices': [0,\n",
      "                                                                          22],\n",
      "                                                              'url': 'http://t.co/lSHJSRwQa1'}]}},\n",
      "                               'favourites_count': 128,\n",
      "                               'follow_request_sent': False,\n",
      "                               'followers_count': 5320,\n",
      "                               'following': False,\n",
      "                               'friends_count': 269,\n",
      "                               'geo_enabled': True,\n",
      "                               'has_extended_profile': False,\n",
      "                               'id': 108564136,\n",
      "                               'id_str': '108564136',\n",
      "                               'is_translation_enabled': False,\n",
      "                               'is_translator': False,\n",
      "                               'lang': 'en',\n",
      "                               'listed_count': 210,\n",
      "                               'location': 'London-Hong Kong-Washington DC',\n",
      "                               'name': 'IAM',\n",
      "                               'notifications': False,\n",
      "                               'profile_background_color': 'DFDFDF',\n",
      "                               'profile_background_image_url': 'http://pbs.twimg.com/profile_background_images/536860588987514881/whGihSNG.png',\n",
      "                               'profile_background_image_url_https': 'https://pbs.twimg.com/profile_background_images/536860588987514881/whGihSNG.png',\n",
      "                               'profile_background_tile': False,\n",
      "                               'profile_banner_url': 'https://pbs.twimg.com/profile_banners/108564136/1416832322',\n",
      "                               'profile_image_url': 'http://pbs.twimg.com/profile_images/536860397869887488/ddZRusGP_normal.png',\n",
      "                               'profile_image_url_https': 'https://pbs.twimg.com/profile_images/536860397869887488/ddZRusGP_normal.png',\n",
      "                               'profile_link_color': 'F11B23',\n",
      "                               'profile_sidebar_border_color': 'FFFFFF',\n",
      "                               'profile_sidebar_fill_color': 'DDEEF6',\n",
      "                               'profile_text_color': '333333',\n",
      "                               'profile_use_background_image': True,\n",
      "                               'protected': False,\n",
      "                               'screen_name': 'IAM_magazine',\n",
      "                               'statuses_count': 15380,\n",
      "                               'time_zone': 'London',\n",
      "                               'translator_type': 'none',\n",
      "                               'url': 'http://t.co/lSHJSRwQa1',\n",
      "                               'utc_offset': 3600,\n",
      "                               'verified': False}},\n",
      " 'source': '<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>',\n",
      " 'text': 'RT @IAM_magazine: Exclusive: In major Valley #patent move Google #IP '\n",
      "         'head Allen Lo is joining Facebook $GOOG $FB https://t.co/FiHWRiETq3',\n",
      " 'truncated': False,\n",
      " 'user': {'contributors_enabled': False,\n",
      "          'created_at': 'Wed May 02 22:00:01 +0000 2007',\n",
      "          'default_profile': True,\n",
      "          'default_profile_image': False,\n",
      "          'description': 'Developer, patent lawyer, typo checker. Tweets tend '\n",
      "                         'to be about Apple, IP, faith, truth, beauty, '\n",
      "                         'mundanity. I work for @parallel_tw.',\n",
      "          'entities': {'description': {'urls': []}},\n",
      "          'favourites_count': 8369,\n",
      "          'follow_request_sent': False,\n",
      "          'followers_count': 1257,\n",
      "          'following': False,\n",
      "          'friends_count': 3520,\n",
      "          'geo_enabled': True,\n",
      "          'has_extended_profile': False,\n",
      "          'id': 5725012,\n",
      "          'id_str': '5725012',\n",
      "          'is_translation_enabled': False,\n",
      "          'is_translator': False,\n",
      "          'lang': 'en',\n",
      "          'listed_count': 101,\n",
      "          'location': 'Boston, MA',\n",
      "          'name': 'Michael Saji',\n",
      "          'notifications': False,\n",
      "          'profile_background_color': 'C0DEED',\n",
      "          'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
      "          'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
      "          'profile_background_tile': False,\n",
      "          'profile_banner_url': 'https://pbs.twimg.com/profile_banners/5725012/1394588981',\n",
      "          'profile_image_url': 'http://pbs.twimg.com/profile_images/613751130439286784/neIdTi5y_normal.png',\n",
      "          'profile_image_url_https': 'https://pbs.twimg.com/profile_images/613751130439286784/neIdTi5y_normal.png',\n",
      "          'profile_link_color': '1DA1F2',\n",
      "          'profile_sidebar_border_color': 'C0DEED',\n",
      "          'profile_sidebar_fill_color': 'DDEEF6',\n",
      "          'profile_text_color': '333333',\n",
      "          'profile_use_background_image': True,\n",
      "          'protected': False,\n",
      "          'screen_name': 'saji',\n",
      "          'statuses_count': 12722,\n",
      "          'time_zone': 'Eastern Time (US & Canada)',\n",
      "          'translator_type': 'none',\n",
      "          'url': None,\n",
      "          'utc_offset': -14400,\n",
      "          'verified': False}}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(json.loads(lines[0].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO use all twitter files\n",
    "data = defaultdict(dict)\n",
    "i=0\n",
    "for line in lines:\n",
    "    tweet = json.loads(line.strip())\n",
    "    if 'text' in tweet: # only messages contains 'text' field is a tweet\n",
    "        ts = time.strptime(tweet['created_at'],'%a %b %d %H:%M:%S +0000 %Y')\n",
    "        data[i][\"time\"] = time.mktime(ts)  \n",
    "        data[i][\"text\"] = tweet['text']\n",
    "    if 'urls' in tweet['entities']:\n",
    "        #print tweet['entities']['urls']\n",
    "        data[i][\"urls\"] = len(tweet['entities']['urls'])\n",
    "    if 'hashtags' in tweet['entities']:\n",
    "        data[i][\"hashtags\"] = len(tweet['entities']['hashtags'])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 1502845106.0, 'text': 'RT @IAM_magazine: Exclusive: In major Valley #patent move Google #IP head Allen Lo is joining Facebook $GOOG $FB https://t.co/FiHWRiETq3', 'urls': 1, 'hashtags': 2}\n",
      "{'time': 1502844958.0, 'text': 'RT @arnabch01: #investors massive bubble in #tech be careful $AAPL $GOOG $MSFT $AMZN $FB $NFLX $TSLA $CSCO $INTC $NVDA $ZNGA $ORCL $JD $MU…', 'urls': 0, 'hashtags': 2}\n"
     ]
    }
   ],
   "source": [
    "print (data[0])\n",
    "print (data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working with text\n",
    "#tokenizer for tweets\n",
    "tknzr = TweetTokenizer(strip_handles=True) #(strip_handles=True, reduce_len=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "corpus = []\n",
    "for i, info in data.copy().items():  \n",
    "    text = info['text'].lower() #.encode('utf-8').decode('ascii','ignore') # content of the tweet\n",
    "    text = re.sub(\"http\\S*\", '', text) # remove urls\n",
    "    text = re.sub(\"^rt\", '', text) # remove rt\n",
    "    text = text.replace('#', '') # remove hashtag\n",
    "    text = re.sub('[^\\w\\s]', '', text) # remove all non-space and non-[a-zA-Z0-9_] characters\n",
    "    text = re.sub('\\d+', '', text)  # remove all numbers\n",
    "    \n",
    "    # REMOVING MISSPELLINGS WITH MULTIPLE CONTINUOUS LETTERS!\n",
    "    text = re.sub('(.)\\1+', '\\1\\1', text)\n",
    "    words = tknzr.tokenize(text)\n",
    "    \n",
    "    text = \" \".join(words) # .encode('utf-8')\n",
    "\n",
    "    # REMOVING DUPLICATES!\n",
    "    if text in corpus:\n",
    "        del data[i]\n",
    "        continue\n",
    "    \n",
    "    corpus.append(text)\n",
    "    data[i]['text'] = text\n",
    "    data[i]['exclamations'] = words.count('!')\n",
    "    data[i]['questions'] = words.count('?')\n",
    "    data[i]['dollar'] = words.count('$')\n",
    "    data[i]['num_words'] = len(text) \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 1502845106.0, 'text': 'iam_magazine exclusive in major valley patent move google ip head allen lo is joining facebook goog fb', 'urls': 1, 'hashtags': 2, 'exclamations': 0, 'questions': 0, 'dollar': 0, 'num_words': 102}\n",
      "{'time': 1502844958.0, 'text': 'arnabch investors massive bubble in tech be careful aapl goog msft amzn fb nflx tsla csco intc nvda znga orcl jd mu', 'urls': 0, 'hashtags': 2, 'exclamations': 0, 'questions': 0, 'dollar': 0, 'num_words': 115}\n",
      "8415\n"
     ]
    }
   ],
   "source": [
    "print (data[0])\n",
    "print (data[1])\n",
    "print (len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>urls</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>exclamations</th>\n",
       "      <th>questions</th>\n",
       "      <th>dollar</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.415000e+03</td>\n",
       "      <td>8415.000000</td>\n",
       "      <td>8415.000000</td>\n",
       "      <td>8415.0</td>\n",
       "      <td>8415.0</td>\n",
       "      <td>8415.0</td>\n",
       "      <td>8415.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.504406e+09</td>\n",
       "      <td>0.698633</td>\n",
       "      <td>0.525015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.985027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.290507e+05</td>\n",
       "      <td>0.542220</td>\n",
       "      <td>1.302777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.010856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.502506e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.503618e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.504391e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.505260e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.505956e+09</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               time         urls     hashtags  exclamations  questions  \\\n",
       "count  8.415000e+03  8415.000000  8415.000000        8415.0     8415.0   \n",
       "mean   1.504406e+09     0.698633     0.525015           0.0        0.0   \n",
       "std    9.290507e+05     0.542220     1.302777           0.0        0.0   \n",
       "min    1.502506e+09     0.000000     0.000000           0.0        0.0   \n",
       "25%    1.503618e+09     0.000000     0.000000           0.0        0.0   \n",
       "50%    1.504391e+09     1.000000     0.000000           0.0        0.0   \n",
       "75%    1.505260e+09     1.000000     0.000000           0.0        0.0   \n",
       "max    1.505956e+09     3.000000    12.000000           0.0        0.0   \n",
       "\n",
       "       dollar    num_words  \n",
       "count  8415.0  8415.000000  \n",
       "mean      0.0    79.985027  \n",
       "std       0.0    28.010856  \n",
       "min       0.0     4.000000  \n",
       "25%       0.0    61.000000  \n",
       "50%       0.0    82.000000  \n",
       "75%       0.0   102.000000  \n",
       "max       0.0   140.000000  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8415 entries, 0 to 15492\n",
      "Data columns (total 8 columns):\n",
      "time            8415 non-null float64\n",
      "text            8415 non-null object\n",
      "urls            8415 non-null int64\n",
      "hashtags        8415 non-null int64\n",
      "exclamations    8415 non-null int64\n",
      "questions       8415 non-null int64\n",
      "dollar          8415 non-null int64\n",
      "num_words       8415 non-null int64\n",
      "dtypes: float64(1), int64(6), object(1)\n",
      "memory usage: 591.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text features based on frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates\n",
    "df = df.drop_duplicates(subset=['text'], keep=False)\n",
    "df.describe()\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          frequency\n",
      "count  12704.000000\n",
      "mean       6.499292\n",
      "std       39.621714\n",
      "min        1.000000\n",
      "25%        1.000000\n",
      "50%        2.000000\n",
      "75%        4.000000\n",
      "max     1631.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>__</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>___</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_anthrobear</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_ayouba_</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_free_</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_jackmohr</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_ms_izzy</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_rone</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_seandavid</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_thethletter</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              frequency\n",
       "__                    1\n",
       "___                   1\n",
       "_anthrobear           1\n",
       "_ayouba_              1\n",
       "_free_                1\n",
       "_jackmohr             1\n",
       "_ms_izzy              1\n",
       "_rone                 1\n",
       "_seandavid            4\n",
       "_thethletter          1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "word_vectorizer = CountVectorizer(analyzer='word', stop_words='english')\n",
    "sparse_matrix = word_vectorizer.fit_transform(df['text'])\n",
    "frequencies = sum(sparse_matrix).toarray()[0]\n",
    "words = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\n",
    "print (words.describe())\n",
    "words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smaller dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         frequency\n",
      "count  6368.000000\n",
      "mean     11.964353\n",
      "std      55.427568\n",
      "min       2.000000\n",
      "25%       2.000000\n",
      "50%       4.000000\n",
      "75%       8.000000\n",
      "max    1631.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>_seandavid</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aa</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaba</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aal</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaoi</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aap</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aapl</th>\n",
       "      <td>1631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aapls</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaron</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ab</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            frequency\n",
       "_seandavid          4\n",
       "aa                 17\n",
       "aaba                5\n",
       "aal                 8\n",
       "aaoi                7\n",
       "aap                 9\n",
       "aapl             1631\n",
       "aapls              11\n",
       "aaron               2\n",
       "ab                  3"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer = CountVectorizer(analyzer='word', stop_words='english',min_df=2, max_df=3000)\n",
    "sparse_matrix = word_vectorizer.fit_transform(df['text'])\n",
    "frequencies = sum(sparse_matrix).toarray()[0]\n",
    "words = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\n",
    "print (words.describe())\n",
    "words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding structure in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "del words\n",
    "#create data_samples\n",
    "#data_samples= [t['text'] for t in data.values()]\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_seandavid', 'aa', 'aaba', 'aal', 'aaoi', 'aap', 'aapls', 'aaron', 'ab', 'abb', 'abbv', 'abc', 'abco', 'abeo', 'abil', 'ability', 'able', 'abnormalreturns', 'aboard', 'abou', 'absolutely', 'abt', 'abvg', 'abx', 'academy', 'acadian', 'acanal', 'accelerates', 'accelerating', 'accelerator', 'acceleratorincubator', 'accenture', 'accept', 'access', 'accessories', 'according', 'accordingly', 'account', 'accounting', 'accounts']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_features = 250\n",
    "n_components = 10\n",
    "n_top_words = 10\n",
    "\n",
    "\n",
    "### Counts\n",
    "tf_vectorizer = CountVectorizer(min_df=2, max_df=1000, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(df.text)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print (tf_feature_names[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF as text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=2,max_df=1000,stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(df.text)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding topics with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: new iphone ibm sell calls google bullish vs amp year\n",
      "Topic #1: ibm shares business international machines corporation week amp sells look\n",
      "Topic #2: ibm spy nflx twtr crm nvda qqq wmt brk amd\n",
      "Topic #3: apple just amazon day short high going buy getting need\n",
      "Topic #4: box twtr googl buy stocks check ai nice soon coming\n",
      "Topic #5: management position microsoft alphabet llc stake million today corporation capital\n",
      "Topic #6: trading tech googl amp long company free chart nflx bitcoin\n",
      "Topic #7: market baba box googl options amp earnings spx car value\n",
      "Topic #8: tesla big read ibm holdings trade price good cloud model\n",
      "Topic #9: stocks stock time investing news snapchat like stockmarket says money\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n_components, learning_method='online')\n",
    "lda.fit(tf) ## fitting counts\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: know going hit need highs people timothysykes love launch articles\n",
      "Topic #1: calls look cars buying electric sep gm day analysts future\n",
      "Topic #2: spy nflx googl stocks free qqq market twtr stock nvda\n",
      "Topic #3: ibm read tesla new box price blockchain check ai model\n",
      "Topic #4: getting group trades ive largest great th strong amazing sell\n",
      "Topic #5: box buy rating lol earnings hold weekly september stock lets\n",
      "Topic #6: facebook googl box google data partners options value way car\n",
      "Topic #7: good amazon high time stocks trade year snapchat chart close\n",
      "Topic #8: holdings ibm twtr stocks big crm brk agn amd baba\n",
      "Topic #9: corporation shares management business position llc microsoft alphabet stake international\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n_components, learning_method='online')\n",
    "lda.fit(tfidf) ## fitting tf-idf counts\n",
    "print_top_words(lda, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the LDAs here with those obtained from the smaller tweet database, we don't see much discernible change. Maybe, if we include more data, the information could become clearer!\n",
    "\n",
    "## Simple sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>text</th>\n",
       "      <th>urls</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>exclamations</th>\n",
       "      <th>questions</th>\n",
       "      <th>dollar</th>\n",
       "      <th>num_words</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>iam_magazine exclusive in major valley patent ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>arnabch investors massive bubble in tech be ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>nyinvesting google goog is the embodiment of m...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>greenstocks timberr iwm spy tlt gs gld btc goo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>bank of nova scotia buys shares of alphabet in...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>alphabet inc goog stake raised by north star a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>themotleyfool the machines keep getting smarte...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.502844e+09</td>\n",
       "      <td>as alphabet goog valuation rose robshaw amp ju...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.502844e+09</td>\n",
       "      <td>warren averett asset management llc boosts pos...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.502844e+09</td>\n",
       "      <td>goog himx vuzi great article</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           time                                               text  urls  \\\n",
       "0  1.502845e+09  iam_magazine exclusive in major valley patent ...     1   \n",
       "1  1.502845e+09  arnabch investors massive bubble in tech be ca...     0   \n",
       "2  1.502845e+09  nyinvesting google goog is the embodiment of m...     0   \n",
       "3  1.502845e+09  greenstocks timberr iwm spy tlt gs gld btc goo...     0   \n",
       "4  1.502845e+09  bank of nova scotia buys shares of alphabet in...     1   \n",
       "5  1.502845e+09  alphabet inc goog stake raised by north star a...     1   \n",
       "6  1.502845e+09  themotleyfool the machines keep getting smarte...     1   \n",
       "7  1.502844e+09  as alphabet goog valuation rose robshaw amp ju...     0   \n",
       "8  1.502844e+09  warren averett asset management llc boosts pos...     1   \n",
       "9  1.502844e+09                       goog himx vuzi great article     1   \n",
       "\n",
       "   hashtags  exclamations  questions  dollar  num_words  positive  negative  \n",
       "0         2             0          0       0        102         0         0  \n",
       "1         2             0          0       0        115         0         0  \n",
       "2         6             0          0       0        114         2         1  \n",
       "3         0             0          0       0        106         0         0  \n",
       "4         0             0          0       0         52         0         0  \n",
       "5         0             0          0       0         65         0         0  \n",
       "6         0             0          0       0         94         2         0  \n",
       "7         0             0          0       0         88         0         0  \n",
       "8         0             0          0       0         72         0         0  \n",
       "9         0             0          0       0         28         1         0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive = pd.read_csv('positive-words.txt', names=['a'], encoding = \"ISO-8859-1\")\n",
    "positive = set(positive['a'].tolist())\n",
    "negative = pd.read_csv('negative-words.txt', names=['a'], encoding = \"ISO-8859-1\")\n",
    "negative = set(negative['a'].tolist())\n",
    "\n",
    "count_positive = []\n",
    "count_negative = []\n",
    "for i, row in df.iterrows():\n",
    "    commonp = set(row['text'].split()).intersection(positive) \n",
    "    count_positive.append(len(commonp))\n",
    "    commonn = set(row['text'].split()).intersection(negative) \n",
    "    count_negative.append(len(commonn))\n",
    "\n",
    "df['positive'] = count_positive\n",
    "df['negative'] = count_negative\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>urls</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>exclamations</th>\n",
       "      <th>questions</th>\n",
       "      <th>dollar</th>\n",
       "      <th>num_words</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.415000e+03</td>\n",
       "      <td>8415.000000</td>\n",
       "      <td>8415.000000</td>\n",
       "      <td>8415.0</td>\n",
       "      <td>8415.0</td>\n",
       "      <td>8415.0</td>\n",
       "      <td>8415.000000</td>\n",
       "      <td>8415.000000</td>\n",
       "      <td>8415.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.504406e+09</td>\n",
       "      <td>0.698633</td>\n",
       "      <td>0.525015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.985027</td>\n",
       "      <td>0.423173</td>\n",
       "      <td>0.273678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.290507e+05</td>\n",
       "      <td>0.542220</td>\n",
       "      <td>1.302777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.010856</td>\n",
       "      <td>0.666000</td>\n",
       "      <td>0.565290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.502506e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.503618e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.504391e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.505260e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.505956e+09</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               time         urls     hashtags  exclamations  questions  \\\n",
       "count  8.415000e+03  8415.000000  8415.000000        8415.0     8415.0   \n",
       "mean   1.504406e+09     0.698633     0.525015           0.0        0.0   \n",
       "std    9.290507e+05     0.542220     1.302777           0.0        0.0   \n",
       "min    1.502506e+09     0.000000     0.000000           0.0        0.0   \n",
       "25%    1.503618e+09     0.000000     0.000000           0.0        0.0   \n",
       "50%    1.504391e+09     1.000000     0.000000           0.0        0.0   \n",
       "75%    1.505260e+09     1.000000     0.000000           0.0        0.0   \n",
       "max    1.505956e+09     3.000000    12.000000           0.0        0.0   \n",
       "\n",
       "       dollar    num_words     positive     negative  \n",
       "count  8415.0  8415.000000  8415.000000  8415.000000  \n",
       "mean      0.0    79.985027     0.423173     0.273678  \n",
       "std       0.0    28.010856     0.666000     0.565290  \n",
       "min       0.0     4.000000     0.000000     0.000000  \n",
       "25%       0.0    61.000000     0.000000     0.000000  \n",
       "50%       0.0    82.000000     0.000000     0.000000  \n",
       "75%       0.0   102.000000     1.000000     0.000000  \n",
       "max       0.0   140.000000     6.000000     4.000000  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6      themotleyfool the machines keep getting smarte...\n",
       "9                           goog himx vuzi great article\n",
       "10     robertrelder apples bargaining power rising go...\n",
       "14     arnabch ai robotics bigdata genomics stemcell ...\n",
       "15     applewatch to support both lte and nonlte mode...\n",
       "20     beijing transit contactless mpayment system ex...\n",
       "26     tweaktown pr asrockinfo introduces the x iot r...\n",
       "29     pr asrockinfo introduces the x iot router for ...\n",
       "41     stocktwits since its ipo home depot is actuall...\n",
       "45     edborgato amzns same day pick up locations are...\n",
       "49     would be amazed if jana partners manage to sel...\n",
       "56           gs aapl amzn need to lead us higher spx dji\n",
       "58     active traders try one of these free trading g...\n",
       "59     xplr join us for play by play action on stocks...\n",
       "60     amzn pzza restaurants are in a tech race to ma...\n",
       "68     amzn part bmark offering guidance y y y y y y ...\n",
       "73     hot options alert midday tuesday august bac dk...\n",
       "77     there is a chance apple could be making a doub...\n",
       "80     join robinhoodapp and well both get a share of...\n",
       "81     retail never learns when buying stock you dont...\n",
       "84     energyandcapita why investors should love the ...\n",
       "86     tsla has traded volume of yesterday and nearly...\n",
       "88     a collection of testimonials from satisfied su...\n",
       "89     why investors should love the new tsla semitru...\n",
       "93     lisahopeking dont be too proud to copy is fb i...\n",
       "94     fb new facebook data center a boost to ohios t...\n",
       "96     amazing tandemtrader is amazing if you want to...\n",
       "98     boost your focus with these simple exercises y...\n",
       "108    the real reason ibm is like a utility csco d d...\n",
       "124    china lchaim make me a match ginni cant even d...\n",
       "138    arnabch quantum computing t revolutionize ai m...\n",
       "140                           ibm a good opportunity ibm\n",
       "143    barronstechblog amazon baird likes hulu win ex...\n",
       "152    pretty basic set up aezs lets see what where s...\n",
       "157    ltea huge news out mybeststock stockinfotv frs...\n",
       "162    af_singledad digaf wake up people grab you a s...\n",
       "163    aapl fb twtr snap googl tech companies urge su...\n",
       "166    digaf wake up people grab you a sure fire winn...\n",
       "170    androsform opening lines id be looking at righ...\n",
       "186    top internet stocks on the market amzn ebay gr...\n",
       "187    cah bandicoot available for ps and box buy now...\n",
       "190    first trust advisors lp raises stake in box in...\n",
       "203    jp marvel investment advisors llc has stake in...\n",
       "207    matrix_trade full free analysis posted on fang...\n",
       "209    free make more money with us deals cash dowjon...\n",
       "212    full free analysis posted on fang fb amzn nflx...\n",
       "214    community trust amp investment co raises stake...\n",
       "235    community trust amp investment co has stake in...\n",
       "240    realdonaldtrump amzn more powerful than you re...\n",
       "242    studenttradeaid dailymarketpoll who will win t...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['positive'] >0) & (df['negative']  == 0)]['text'].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18     goog neonazi group moves to dark web after web...\n",
       "30     arnabch will advances in ai ml robotics nanote...\n",
       "33     arnabch hpc ai ml bigdata may soon enable geno...\n",
       "39     discussing the retail landscape department sto...\n",
       "48     sitrep risk on mrk ceo youre fired amzn gs leg...\n",
       "54     amzn aap wmt amazon will probably go onto crus...\n",
       "55     dont worry about how many shares you can buy c...\n",
       "67     thestreet amazon will probably go onto crush a...\n",
       "83     tsla sa another risk factor for tesla shorts d...\n",
       "99     microsoft acquires cloudcomputing orchestratio...\n",
       "115    international business machines ibm fall to no...\n",
       "117             the blue cloud collapses i told u ibm so\n",
       "121    jimcramer mariabartiromo so u wont ask ginni a...\n",
       "122    seekingalpha ibm watson disappointment risks f...\n",
       "123    ibm watson disappointment risks further downwa...\n",
       "128    china big market thus saith ginni so far zero ...\n",
       "131    marketsupchuck is ibms dividend yield killing ...\n",
       "132    is ibms dividend yield killing strategic imper...\n",
       "149    stocknewsdotcom snap cantor fitzgerald sees al...\n",
       "150    snap cantor fitzgerald sees almost upside for ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['positive'] ==0) & (df['negative']  > 0)]['text'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets: 8415\n",
      "Total tweets positive: 2212\n",
      "Total tweets negative: 1227\n",
      "Tweets with no info: 4339\n",
      "neutral tweets: 637\n"
     ]
    }
   ],
   "source": [
    "print (\"Total tweets:\", len(df))\n",
    "print (\"Total tweets positive:\",len(df[(df['positive'] >0) & (df['negative']  == 0)]))\n",
    "print (\"Total tweets negative:\",len(df[(df['positive'] == 0) & (df['negative']  > 0)]))\n",
    "print (\"Tweets with no info:\", len(df[(df['positive'] == 0) & (df['negative']  == 0)]))\n",
    "print (\"neutral tweets:\", len(df[(df['positive'] >0) & (df['negative']  > 0)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Similarity using word embedings (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting each tweet into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "matrix = []\n",
    "filtered = []\n",
    "useless_indx = []\n",
    "counter = 0\n",
    "\n",
    "for i, row in data.items():\n",
    "    filtered_text = [model[w] for w in row['text'].split() if w in model and w not in stop_words]\n",
    "    filtered.append([w for w in row['text'].split() if w in model and w not in stop_words])\n",
    "    if len(filtered_text):\n",
    "        matrix.append(matutils.unitvec(np.array(filtered_text).mean(axis=0)))\n",
    "    else:\n",
    "        useless_indx.append(counter)\n",
    "    counter += 1\n",
    "    \n",
    "df = df.drop(df.index[useless_indx], inplace = False)\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.672333   0.60287177 ... 0.34146225 0.54058195 0.20146088]\n",
      " [0.672333   1.         0.61664081 ... 0.27026728 0.44768505 0.22181737]\n",
      " [0.60287177 0.61664081 1.         ... 0.26864053 0.44867838 0.2980352 ]\n",
      " ...\n",
      " [0.34146225 0.27026728 0.26864053 ... 1.         0.58074919 0.64923257]\n",
      " [0.54058195 0.44768505 0.44867838 ... 0.58074919 1.         0.39800358]\n",
      " [0.20146088 0.22181737 0.2980352  ... 0.64923257 0.39800358 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#computing similarity between tweets\n",
    "\n",
    "matrix = np.array(matrix)\n",
    "sim = np.dot(matrix, matrix.transpose())\n",
    "print (sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8411, 8411)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.536826e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.883489e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.193082e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.521175e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.084871e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.881512e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.674592e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "count  3.536826e+07\n",
       "mean   3.883489e-01\n",
       "std    1.193082e-01\n",
       "min   -1.521175e-01\n",
       "25%    3.084871e-01\n",
       "50%    3.881512e-01\n",
       "75%    4.674592e-01\n",
       "max    1.000000e+00"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reshaping into a data frame\n",
    "print (sim.shape)\n",
    "dup = np.fill_diagonal(sim, 0)\n",
    "\n",
    "simdf = pd.DataFrame(list(sim[np.triu_indices(sim.shape[1], 1)]))\n",
    "simdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dimension of the transformed matrix post word2vec above is 4 less than expected (8411 v/s 8415), I also remove the relevant row items from *df*.\n",
    "\n",
    "### Get the most similar tweets for each sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity: 0.6785147315887967\n",
      "stocktwits since its ipo home depot is actually outperforming amazon compare the green to the yellow line on this\n",
      "['since', 'ipo', 'home', 'depot', 'actually', 'outperforming', 'amazon', 'compare', 'green', 'yellow', 'line']\n",
      "ndygrosso true fb also traded below its ipo price but then reclaimed it over gain since waiting for twtr\n",
      "['remember', 'fb', 'ipo', 'price', 'wanted', 'look', 'going']\n"
     ]
    }
   ],
   "source": [
    "pos = 41\n",
    "most_similar = np.argmax(sim[pos][:])\n",
    "print (\"similarity:\", sim[pos][most_similar])\n",
    "print (df.iloc[pos]['text'])\n",
    "print (filtered[pos])\n",
    "print (df.iloc[most_similar]['text'])\n",
    "print (filtered[most_similar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity: 0.8729076740177735\n",
      "dont worry about how many shares you can buy concern yourself wthe return on those shares stocks amzn googl\n",
      "['dont', 'worry', 'many', 'shares', 'buy', 'concern', 'wthe', 'return', 'shares', 'stocks']\n",
      "dont stay away from stocks with a high share price its okay to only buy a couple of shares stocks amzn googl\n",
      "['need', 'since', 'walmart', 'wmt', 'paid', 'b', 'corporate', 'income', 'tax', 'amazon', 'paid', 'b', 'amazon']\n"
     ]
    }
   ],
   "source": [
    "neg = 55\n",
    "most_similar = np.argmax(sim[neg][:])\n",
    "print (\"similarity:\", sim[neg][most_similar])\n",
    "print (df.iloc[neg]['text'])\n",
    "print (filtered[neg])\n",
    "print (df.iloc[most_similar]['text'])\n",
    "print (filtered[most_similar])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a labelled dataset, and split into train/test\n",
    "\n",
    "The data is slightly imbalanced, since positive-to-negative tweet ratio is ~1.8. But this is not typically the skewed ratio that is cause for concern unlike in fraud detection problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for 100 estimators with max tree depth of 3: 0.821221\n",
      "Accuracy score for 100 estimators with max tree depth of 7: 0.848837\n",
      "Accuracy score for 100 estimators with max tree depth of 10: 0.841570\n",
      "Accuracy score for 300 estimators with max tree depth of 3: 0.869186\n",
      "Accuracy score for 300 estimators with max tree depth of 7: 0.860465\n",
      "Accuracy score for 300 estimators with max tree depth of 10: 0.848837\n",
      "Accuracy score for 500 estimators with max tree depth of 3: 0.876453\n",
      "Accuracy score for 500 estimators with max tree depth of 7: 0.853198\n",
      "Accuracy score for 500 estimators with max tree depth of 10: 0.840116\n"
     ]
    }
   ],
   "source": [
    "df['label'] = 1 * ((df['positive'] >0) & (df['negative']  == 0)) - 1 * ((df['positive'] == 0) & (df['negative']  > 0))\n",
    "df_model = df.drop(columns = ['positive', 'negative', 'exclamations', 'questions', 'dollar', 'text'], inplace = False)\n",
    "df_model = df_model[~((df['positive'] > 0) & (df['negative']  > 0))]    # Removing amiguous/neutral data\n",
    "df_no_info = df_model[df_model['label'] == 0]\n",
    "df_model = df_model[df_model['label'] != 0]\n",
    "\n",
    "matrix = pd.DataFrame(matrix, columns = ['X' + str(x) for x in range(matrix.shape[1])])\n",
    "df_model = df_model.join(matrix, how = 'left')\n",
    "df_no_info = df_no_info.join(matrix, how = 'left')\n",
    "\n",
    "X = df_model.drop(columns = ['label'], inplace = False)\n",
    "y = df_model['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "for n_estimators in [100, 300, 500]:\n",
    "    for max_depth in [3, 7, 10]:\n",
    "        clf = GradientBoostingClassifier(n_estimators = n_estimators,\n",
    "                                         max_depth = max_depth,\n",
    "                                         learning_rate = 0.1)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_predict_class = clf.predict(X_test)\n",
    "        print (\"Accuracy score for %d estimators with max tree depth of %d: %f\"\n",
    "               % (n_estimators, max_depth, accuracy_score(y_test, y_predict_class)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I train a Gradient Boosting model for multiple hyper-parameter combinations to see what would give good OOS results. The best one will be selected for the task of labelling the tweets without any label information. The predicted class can be found in the 'label' column of df_no_info.\n",
    "\n",
    "As we see from above results, as we increase number of trees, keeping small trees in place, the model performs better. Thus, I choose an n_estimators of 1000 and max_depth of 3 below\n",
    "\n",
    "\n",
    "### Classifying tweets with no information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for 1000 estimators with max tree depth of 3: 0.886628\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>time</th>\n",
       "      <th>urls</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>num_words</th>\n",
       "      <th>label</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>...</th>\n",
       "      <th>X290</th>\n",
       "      <th>X291</th>\n",
       "      <th>X292</th>\n",
       "      <th>X293</th>\n",
       "      <th>X294</th>\n",
       "      <th>X295</th>\n",
       "      <th>X296</th>\n",
       "      <th>X297</th>\n",
       "      <th>X298</th>\n",
       "      <th>X299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>102</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.002412</td>\n",
       "      <td>0.012745</td>\n",
       "      <td>-0.009593</td>\n",
       "      <td>0.092464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068362</td>\n",
       "      <td>0.093186</td>\n",
       "      <td>-0.058904</td>\n",
       "      <td>0.054739</td>\n",
       "      <td>-0.014146</td>\n",
       "      <td>-0.089333</td>\n",
       "      <td>0.009702</td>\n",
       "      <td>-0.032968</td>\n",
       "      <td>-0.034036</td>\n",
       "      <td>0.050125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>0.057487</td>\n",
       "      <td>0.035829</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.101208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025843</td>\n",
       "      <td>0.022043</td>\n",
       "      <td>-0.021395</td>\n",
       "      <td>-0.029454</td>\n",
       "      <td>0.013659</td>\n",
       "      <td>-0.083605</td>\n",
       "      <td>-0.048005</td>\n",
       "      <td>-0.015275</td>\n",
       "      <td>-0.077464</td>\n",
       "      <td>0.041782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.063916</td>\n",
       "      <td>0.053886</td>\n",
       "      <td>-0.007517</td>\n",
       "      <td>0.047148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045912</td>\n",
       "      <td>0.088100</td>\n",
       "      <td>-0.050449</td>\n",
       "      <td>0.030778</td>\n",
       "      <td>0.009320</td>\n",
       "      <td>-0.049032</td>\n",
       "      <td>-0.067791</td>\n",
       "      <td>-0.054342</td>\n",
       "      <td>-0.076330</td>\n",
       "      <td>0.069449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.008953</td>\n",
       "      <td>-0.012955</td>\n",
       "      <td>-0.008861</td>\n",
       "      <td>0.060886</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014254</td>\n",
       "      <td>-0.002642</td>\n",
       "      <td>-0.156609</td>\n",
       "      <td>0.008664</td>\n",
       "      <td>0.088631</td>\n",
       "      <td>-0.031812</td>\n",
       "      <td>0.007395</td>\n",
       "      <td>0.056196</td>\n",
       "      <td>0.010431</td>\n",
       "      <td>0.066509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.050434</td>\n",
       "      <td>0.006423</td>\n",
       "      <td>-0.011624</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030207</td>\n",
       "      <td>0.037583</td>\n",
       "      <td>-0.138747</td>\n",
       "      <td>0.028643</td>\n",
       "      <td>0.043147</td>\n",
       "      <td>-0.103074</td>\n",
       "      <td>0.004515</td>\n",
       "      <td>0.023327</td>\n",
       "      <td>-0.008192</td>\n",
       "      <td>-0.009076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 306 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index          time  urls  hashtags  num_words  label        X0        X1  \\\n",
       "0      0  1.502845e+09     1         2        102     -1 -0.002412  0.012745   \n",
       "1      1  1.502845e+09     0         2        115      1  0.057487  0.035829   \n",
       "3      3  1.502845e+09     0         0        106     -1 -0.063916  0.053886   \n",
       "4      4  1.502845e+09     1         0         52      1 -0.008953 -0.012955   \n",
       "5      5  1.502845e+09     1         0         65      1 -0.050434  0.006423   \n",
       "\n",
       "         X2        X3    ...         X290      X291      X292      X293  \\\n",
       "0 -0.009593  0.092464    ...     0.068362  0.093186 -0.058904  0.054739   \n",
       "1  0.000600  0.101208    ...     0.025843  0.022043 -0.021395 -0.029454   \n",
       "3 -0.007517  0.047148    ...     0.045912  0.088100 -0.050449  0.030778   \n",
       "4 -0.008861  0.060886    ...     0.014254 -0.002642 -0.156609  0.008664   \n",
       "5 -0.011624 -0.000014    ...    -0.030207  0.037583 -0.138747  0.028643   \n",
       "\n",
       "       X294      X295      X296      X297      X298      X299  \n",
       "0 -0.014146 -0.089333  0.009702 -0.032968 -0.034036  0.050125  \n",
       "1  0.013659 -0.083605 -0.048005 -0.015275 -0.077464  0.041782  \n",
       "3  0.009320 -0.049032 -0.067791 -0.054342 -0.076330  0.069449  \n",
       "4  0.088631 -0.031812  0.007395  0.056196  0.010431  0.066509  \n",
       "5  0.043147 -0.103074  0.004515  0.023327 -0.008192 -0.009076  \n",
       "\n",
       "[5 rows x 306 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators = 1000, max_depth = 3, learning_rate = 0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_predict_class = clf.predict(X_test)\n",
    "print (\"Accuracy score for 1000 estimators with max tree depth of 3: %f\" % accuracy_score(y_test, y_predict_class))\n",
    "df_no_info['label'] = clf.predict(df_no_info.drop(columns = ['label'], inplace = False))\n",
    "df_no_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the labelled prediction for each tweet in df_no_info. I recommend using something like the GradientBoostedClassifier, since boosting and other ensemble methods tend to outperform generally and specially in cases of imbalanced datasets. We see that the accuracy on the test set for our trained model is ~88.7%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
